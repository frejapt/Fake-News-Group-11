{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "small_data_set = pd.read_csv('https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace URLs\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    text = url_pattern.sub('<URL>', text)\n",
    "\n",
    "    # Replace emails\n",
    "    email_pattern = re.compile(r'\\S+@\\S+')\n",
    "    text = email_pattern.sub('<EMAIL>', text)\n",
    "\n",
    "    # Replace dates (YYYY-MM-DD and DD/MM/YYYY formats)\n",
    "    date_pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2}|\\d{2}/\\d{2}/\\d{4})')\n",
    "    text = date_pattern.sub('<DATE>', text)\n",
    "\n",
    "    # Replace numbers\n",
    "    num_pattern = re.compile(r'\\b\\d+\\b')\n",
    "    text = num_pattern.sub('<NUM>', text)\n",
    "\n",
    "    # Remove punctuation and non-word characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    return text \n",
    "\n",
    "# Create a new column to store the cleaned text\n",
    "small_data_set['content_clean'] = small_data_set['content'].apply(clean_text)\n",
    "\n",
    "# Tokenization\n",
    "small_data_set['content_tokens'] = small_data_set['content_clean'].apply(word_tokenize)\n",
    "\n",
    "# Stemming\n",
    "ps = PorterStemmer()\n",
    "small_data_set['content_stemming'] = small_data_set['content_tokens'].apply(lambda x: [ps.stem(word) for word in x])\n",
    "\n",
    "# Stop word removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "small_data_set['clean_content'] = small_data_set['content_stemming'].apply(lambda x: [word for word in x if word not in stop_words])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the number of unique words before, under and after preproccesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data\n",
    "unique_words_content = set()\n",
    "for text in small_data_set['content']:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    unique_words_content.update(tokens)\n",
    "num_unique_words_content = len(unique_words_content)\n",
    "print(\"Number of unique words in 'content':\", num_unique_words_content)\n",
    "\n",
    "# After stemming\n",
    "unique_words_stemmed_content = set()\n",
    "for tokens in small_data_set['content_stemming']:\n",
    "    unique_words_stemmed_content.update(tokens)\n",
    "num_unique_words_stemmed_content = len(unique_words_stemmed_content)\n",
    "print(\"Number of unique words after stemming:\", num_unique_words_stemmed_content)\n",
    "\n",
    "# Calculate number of unique words in 'clean_content'\n",
    "unique_words_clean_content = set()\n",
    "for tokens in small_data_set['clean_content']:\n",
    "    unique_words_clean_content.update(tokens)\n",
    "num_unique_words_clean_content = len(unique_words_clean_content)\n",
    "print(\"Number of unique words in 'clean_content':\", num_unique_words_clean_content)\n",
    "\n",
    "# calculate reduction rate from raw to stemming\n",
    "red_raw_to_stem = num_unique_words_content - num_unique_words_stemmed_content\n",
    "print(\"Reduction rate from raw data to stemming:\",red_raw_to_stem)\n",
    "\n",
    "red_stem_to_stop = num_unique_words_stemmed_content - num_unique_words_clean_content\n",
    "print(\"Reduction rate from stemming to stopwordremoval:\", red_stem_to_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration of the big dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We split the file into chunks and then concatenate them\n",
    "chunksize = 10000\n",
    "chunks = []\n",
    "for chunk in pd.read_csv('995,000_rows.csv', chunksize=chunksize):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "fake_news = pd.concat(chunks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding missing values in the 'type' column\n",
    "\n",
    "missing_values = fake_news['type'].isnull().sum()\n",
    "print('Number of missing values: ', missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Distribution of the 'type' column\n",
    "\n",
    "ax = fake_news['type'].value_counts().plot(kind='bar')\n",
    "\n",
    "# Setting the x-label, y-label, and title\n",
    "ax.set_xlabel('Type')\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.set_title('Distribution of types')\n",
    "\n",
    "# Formating the y-axis ticks as percentages\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=len(fake_news['type'])))\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifying the news into categories and plotting the distribution of the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classify the news into reliable, fake and other\n",
    "def classify_news(type):\n",
    "    reliable_types = ['reliable', 'political', 'clickbait']\n",
    "    fake_news_types = ['fake', 'hate', 'conspiracy', 'junksci','state','bias']\n",
    "    \n",
    "    if type in reliable_types:\n",
    "        return 'reliable'\n",
    "    elif type in fake_news_types:\n",
    "        return 'fake'\n",
    "    else:\n",
    "        return 'other'\n",
    "    \n",
    "fake_news['news_category'] = fake_news['type'].apply(classify_news)\n",
    "\n",
    "\n",
    "# The distribution of the news categories\n",
    "print(fake_news['news_category'].value_counts())\n",
    "\n",
    "# The distribution of the news categories in percentage\n",
    "print(fake_news['news_category'].value_counts(normalize=True))\n",
    "\n",
    "# Plot the percentage of each category\n",
    "ax = fake_news['news_category'].value_counts().plot(kind='bar')\n",
    "\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.set_title('Distribution of news categories')\n",
    "\n",
    "# Format the y-axis ticks as percentages\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=len(fake_news['news_category'])))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the distribution of the 'type' column in the 'other' category\n",
    "other_category_fakenews = fake_news[fake_news['news_category'] == 'other']\n",
    "print(other_category_fakenews['type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate the keywords\n",
    "print(fake_news['keywords'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "# Count the number of URLs in the 'content' column\n",
    "url_count = fake_news['content'].str.count(url_pattern).sum()\n",
    "\n",
    "print(\"Number of URLs in the 'content' column:\", url_count)\n",
    "\n",
    "#Count dates in the 'content' column\n",
    "\n",
    "email_pattern = re.compile(r'\\S+@\\S+')\n",
    "email_count = fake_news['content'].str.count(email_pattern).sum()\n",
    "print(\"Number of emails in the 'content' column:\", email_count)\n",
    "\n",
    "#Count numbers in the 'content' column\n",
    "num_pattern = re.compile(r'\\b\\d+\\b')\n",
    "num_count = fake_news['content'].str.count(num_pattern).sum()\n",
    "print(\"Number of numbers in the 'content' column:\", num_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the 100 most used words in the dataset before and after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace URLs\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    text = url_pattern.sub('<URL>', text)\n",
    "\n",
    "    # Replace emails\n",
    "    email_pattern = re.compile(r'\\S+@\\S+')\n",
    "    text = email_pattern.sub('<EMAIL>', text)\n",
    "\n",
    "    # Replace dates (YYYY-MM-DD and DD/MM/YYYY formats)\n",
    "    date_pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2}|\\d{2}/\\d{2}/\\d{4})')\n",
    "    text = date_pattern.sub('<DATE>', text)\n",
    "\n",
    "    # Replace numbers\n",
    "    num_pattern = re.compile(r'\\b\\d+\\b')\n",
    "    text = num_pattern.sub('<NUM>', text)\n",
    "\n",
    "    # Remove punctuation and non-word characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    return text \n",
    "\n",
    "# Create a new column to store the cleaned text\n",
    "fake_news['content_clean'] = fake_news['content'].apply(clean_text)\n",
    "\n",
    "# Tokenization\n",
    "fake_news['content_tokens'] = fake_news['content_clean'].apply(word_tokenize)\n",
    "\n",
    "# Stemming\n",
    "ps = PorterStemmer()\n",
    "fake_news['content_stemming'] = fake_news['content_tokens'].apply(lambda x: [ps.stem(word) for word in x])\n",
    "\n",
    "# Stop word removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "fake_news['clean_content'] = fake_news['content_stemming'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "fake_news[['id','domain', 'authors', 'type', 'clean_content']].to_csv('cleaned_fake_news.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Find 100 most frequent word in original content\n",
    "word_counter = Counter()\n",
    "\n",
    "for tokens in fake_news['content']:\n",
    "    if isinstance(tokens, str):\n",
    "        tokens = tokens.split()\n",
    "    elif isinstance(tokens, float):\n",
    "        tokens = str(tokens).split()\n",
    "    word_counter.update(tokens)\n",
    "\n",
    "# Get the 100 most common words\n",
    "most_common_words_100 = word_counter.most_common(100)\n",
    "\n",
    "# Print the 100 most common words\n",
    "for word, count in most_common_words_100:\n",
    "    print(f\"'{word}': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "fake_news = pd.read_csv('cleaned_fake_news.csv')\n",
    "\n",
    "word_counter = Counter()\n",
    "\n",
    "# Update the counter for each word in each token list\n",
    "for tokens in fake_news['clean_content']:\n",
    "    if isinstance(tokens, str):\n",
    "        tokens = tokens.split()\n",
    "    word_counter.update(tokens)\n",
    "\n",
    "# Get the 100 most common words\n",
    "most_common_words_after_cleaning = word_counter.most_common(100)\n",
    "\n",
    "# Print the 100 most common words\n",
    "for word, count in most_common_words_after_cleaning:\n",
    "    print(f\"'{word}': {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the 100 most used words from before and after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the 100 most common words in original words\n",
    "most_common_words_10000 = word_counter.most_common(10000)\n",
    "\n",
    "# Plot the 10000 most common words\n",
    "x_bar = []\n",
    "y_bar = []\n",
    "for i in most_common_words_10000[:100]:\n",
    "    x_bar.append(i[0])\n",
    "    y_bar.append(i[1])\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.barh(x_bar, y_bar)\n",
    "\n",
    "# Set the x-label, y-label, and title\n",
    "plt.xlabel('Word count')\n",
    "plt.ylabel('Word')\n",
    "plt.title('Top 100 Most Common Words')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 100 most common words after cleaning\n",
    "x_bar = []\n",
    "y_bar = []\n",
    "for tokens in most_common_words_after_cleaning[:100]:\n",
    "    x_bar.append(tokens[0])\n",
    "    y_bar.append(tokens[1])\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.barh(x_bar, y_bar)\n",
    "\n",
    "plt.xlabel('Word count')\n",
    "plt.ylabel('Word')\n",
    "plt.title('Top 100 Most Common Words')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('cleaned_fake_news.csv')\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#Classify the news into reliable, fake and other\n",
    "def classify_news(type):\n",
    "    reliable_types = ['reliable', 'political', 'clickbait']\n",
    "    fake_news_types = ['fake', 'hate', 'conspiracy', 'junksci','state','bias']\n",
    "    \n",
    "    if type in reliable_types:\n",
    "        return 'reliable'\n",
    "    elif type in fake_news_types:\n",
    "        return 'fake'\n",
    "    else:\n",
    "        return 'other'\n",
    "    \n",
    "df['news_category'] = df['type'].apply(classify_news)\n",
    "\n",
    "print(df['news_category'].value_counts())\n",
    "\n",
    "#Remove 'other' category\n",
    "df = df[df.news_category != 'other']\n",
    "\n",
    "print(df['news_category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming and splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CountVectorizer to convert text data to numbers\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(df['clean_content'])\n",
    "y = df['news_category']\n",
    "\n",
    "y_binary = y.apply(lambda x: 1 if x == 'reliable' else 0)\n",
    "\n",
    "# Split the data into a training, validation, test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y_binary, test_size=0.1, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=1/9, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We try logistic regression\n",
    "\n",
    "model_log = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model_log.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels\n",
    "y_val_pred = model_log.predict(X_val)\n",
    "\n",
    "# Print the accuracy\n",
    "acc_val = accuracy_score(y_val, y_val_pred)\n",
    "print('Validation accuracy:', acc_val)\n",
    "\n",
    "# Print the confusion matrix\n",
    "con_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "print(con_matrix)\n",
    "\n",
    "ConfusionMatrixDisplay(con_matrix, display_labels=model_log.classes_).plot(values_format='d')\n",
    "\n",
    "# Print the classification report\n",
    "class_report = classification_report(y_val, y_val_pred)\n",
    "print(class_report)\n",
    "\n",
    "f1_score_log = f1_score(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We try a naive bayes model\n",
    "\n",
    "model_nb = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "model_nb.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels\n",
    "y_val_pred = model_nb.predict(X_val)\n",
    "\n",
    "# Print the accuracy\n",
    "acc_val_nb = accuracy_score(y_val, y_val_pred)\n",
    "print('Validation accuracy:', acc_val_nb)\n",
    "\n",
    "# Print the confusion matrix\n",
    "con_matrix_nb = confusion_matrix(y_val, y_val_pred)\n",
    "print(con_matrix_nb)\n",
    "ConfusionMatrixDisplay(con_matrix_nb, display_labels=model_nb.classes_).plot(values_format='d')\n",
    "\n",
    "# Print the classification report\n",
    "class_report_nb = classification_report(y_val, y_val_pred)\n",
    "print(class_report_nb)\n",
    "\n",
    "f1_score_nb = f1_score(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We try support a support vector machine model\n",
    "model_svm = LinearSVC()\n",
    "\n",
    "# Train the model\n",
    "model_svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels\n",
    "y_val_pred_svm = model_svm.predict(X_val)\n",
    "\n",
    "# Print the accuracy\n",
    "acc_val_svm = accuracy_score(y_val, y_val_pred_svm)\n",
    "print('Validation accuracy:', acc_val_svm)\n",
    "\n",
    "class_report_svm = classification_report(y_val, y_val_pred_svm)\n",
    "print(class_report_svm)\n",
    "\n",
    "conf_matrix_svm = confusion_matrix(y_val, y_val_pred_svm)\n",
    "print(conf_matrix_svm)\n",
    "\n",
    "ConfusionMatrixDisplay(conf_matrix_svm, display_labels=model_svm.classes_).plot(values_format='d')\n",
    "f1_score_svm = f1_score(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating on the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We evaluate on the test set\n",
    "# Predict the labels\n",
    "y_test_pred_log = model_log.predict(X_test)\n",
    "y_test_pred_nb = model_nb.predict(X_test)\n",
    "y_test_pred_svm = model_svm.predict(X_test)\n",
    "\n",
    "#Accurary on the test set\n",
    "acc_test_log = accuracy_score(y_test, y_test_pred_log)\n",
    "acc_test_nb = accuracy_score(y_test, y_test_pred_nb)\n",
    "acc_test_svm = accuracy_score(y_test, y_test_pred_svm)\n",
    "\n",
    "#F1 score on the test set\n",
    "f_1_test_log = f1_score(y_test, y_test_pred_log)\n",
    "f_1_test_nb = f1_score(y_test, y_test_pred_nb)\n",
    "f_1_test_svm = f1_score(y_test, y_test_pred_svm)\n",
    "\n",
    "#Confusion matrix on the test set\n",
    "conf_matrix_test_log = confusion_matrix(y_test, y_test_pred_log)\n",
    "conf_matrix_test_nb = confusion_matrix(y_test, y_test_pred_nb)\n",
    "conf_matrix_test_svm = confusion_matrix(y_test, y_test_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table of results on test set\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Naive Bayes', 'Support Vector Machine'],\n",
    "    'Validation Accuracy': [acc_val, acc_val_nb, acc_val_svm],\n",
    "    'Test Accuracy': [acc_test_log, acc_test_nb, acc_test_svm],\n",
    "    'F1 Score': [f1_score_log, f1_score_nb, f1_score_svm],\n",
    "    'F1 Score Test': [f_1_test_log, f_1_test_nb, f_1_test_svm]\n",
    "})\n",
    "# Display the results in a table\n",
    "from IPython.display import display\n",
    "display(results)\n",
    "\n",
    "#Plot the confusion matrices\n",
    "\n",
    "ConfusionMatrixDisplay(conf_matrix_test_log, display_labels=model_log.classes_).plot(values_format='d')\n",
    "plt.title('Logistic Regression')\n",
    "ConfusionMatrixDisplay(conf_matrix_test_nb, display_labels=model_nb.classes_).plot(values_format='d')\n",
    "plt.title('Naive Bayes')\n",
    "ConfusionMatrixDisplay(conf_matrix_test_svm, display_labels=model_svm.classes_).plot(values_format='d')\n",
    "plt.title('Support Vector Machine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by cleaning the liar testset, the same way as the fake_news dataset. As we have observed that it gives better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Liar = pd.read_csv('test.tsv', sep='\\t')\n",
    "print(Liar.columns)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace URLs\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    text = url_pattern.sub('<URL>', text)\n",
    "\n",
    "    # Replace emails\n",
    "    email_pattern = re.compile(r'\\S+@\\S+')\n",
    "    text = email_pattern.sub('<EMAIL>', text)\n",
    "\n",
    "    # Replace dates (YYYY-MM-DD and DD/MM/YYYY formats)\n",
    "    date_pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2}|\\d{2}/\\d{2}/\\d{4})')\n",
    "    text = date_pattern.sub('<DATE>', text)\n",
    "\n",
    "    # Replace numbers\n",
    "    num_pattern = re.compile(r'\\b\\d+\\b')\n",
    "    text = num_pattern.sub('<NUM>', text)\n",
    "\n",
    "    # Remove punctuation and non-word characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    return text \n",
    "\n",
    "# Create a new column to store the cleaned text\n",
    "Liar['content_clean'] = Liar['Building a wall on the U.S.-Mexico border will take literally years.'].apply(clean_text)\n",
    "\n",
    "# Tokenization\n",
    "Liar['content_tokens'] = Liar['content_clean'].apply(word_tokenize)\n",
    "\n",
    "# Stemming\n",
    "ps = PorterStemmer()\n",
    "Liar['content_stemming'] = Liar['content_tokens'].apply(lambda x: [ps.stem(word) for word in x])\n",
    "\n",
    "# Stop word removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "Liar['clean_content'] = Liar['content_stemming'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "Liar[[ 'true', 'clean_content']].to_csv('cleaned_Liar_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counter = Counter()\n",
    "\n",
    "# Update the counter for each word in each token list\n",
    "for tokens in Liar['clean_content']:\n",
    "    if isinstance(tokens, str):\n",
    "        tokens = tokens.split()\n",
    "    word_counter.update(tokens)\n",
    "\n",
    "# Get the 100 most common words\n",
    "most_common_words_after_cleaning = word_counter.most_common(100)\n",
    "\n",
    "# Print the 100 most common words\n",
    "for word, count in most_common_words_after_cleaning:\n",
    "    print(f\"'{word}': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "Liar = pd.read_csv('test.tsv', sep='\\t')\n",
    "\n",
    "#Define the function to classify news\n",
    "def classify_news(type):\n",
    "    reliable_types = ['true']\n",
    "    fake_news_types = ['false', 'pants-fire']\n",
    "    \n",
    "    if type in reliable_types:\n",
    "       return 'reliable'\n",
    "    elif type in fake_news_types:\n",
    "       return 'fake'\n",
    "    else:\n",
    "       return 'other'\n",
    "    \n",
    "# Apply the classification function to create a new column 'news_category'\n",
    "Liar['news_category'] = Liar['true'].apply(classify_news)  \n",
    "\n",
    "# Print the distribution of news categories\n",
    "print(Liar['news_category'].value_counts())\n",
    "\n",
    "Liar = Liar[Liar.news_category != 'other']\n",
    "\n",
    "print(Liar['news_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Read the CSV file\n",
    "liar = pd.read_csv('cleaned_Liar_test.csv')\n",
    "\n",
    "#Define the function to classify news\n",
    "def classify_news_liar(type):\n",
    "    reliable_types = ['true']\n",
    "    fake_news_types = ['false', 'pants-fire']\n",
    "    \n",
    "    if type in reliable_types:\n",
    "       return 'reliable'\n",
    "    elif type in fake_news_types:\n",
    "       return 'fake'\n",
    "    else:\n",
    "       return 'other'\n",
    "    \n",
    "# Apply the classification function to create a new column 'news_category'\n",
    "liar['news_category'] = liar['true'].apply(classify_news_liar)  \n",
    "\n",
    "# Print the distribution of news categories\n",
    "print(liar['news_category'].value_counts())\n",
    "\n",
    "#remove 'other' category\n",
    "liar = liar[liar.news_category != 'other']\n",
    "\n",
    "# Test the model on the Liar dataset\n",
    "X_liar = vectorizer.transform(liar['clean_content'])\n",
    "y_liar = liar['news_category']\n",
    "\n",
    "y_liar_binary = y_liar.apply(lambda x: 1 if x == 'reliable' else 0)\n",
    "\n",
    "# Predict the labels\n",
    "y_liar_pred_log = model_log.predict(X_liar)\n",
    "y_liar_pred_svm = model_svm.predict(X_liar)\n",
    "\n",
    "# Calculate the accuracy\n",
    "acc_liar_log = accuracy_score(y_liar_binary, y_liar_pred_log)\n",
    "acc_liar_svm = accuracy_score(y_liar_binary, y_liar_pred_svm)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1_liar_log = f1_score(y_liar_binary, y_liar_pred_log)\n",
    "f1_liar_svm = f1_score(y_liar_binary, y_liar_pred_svm)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix_liar_log = confusion_matrix(y_liar_binary, y_liar_pred_log)\n",
    "conf_matrix_liar_svm = confusion_matrix(y_liar_binary, y_liar_pred_svm)\n",
    "\n",
    "# Display the results in a table\n",
    "results_liar = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Support Vector Machine'],\n",
    "    'Accuracy': [acc_liar_log, acc_liar_svm],\n",
    "    'F1 Score': [f1_liar_log, f1_liar_svm]\n",
    "})\n",
    "display(results_liar)\n",
    "\n",
    "# Plot the confusion matrices\n",
    "ConfusionMatrixDisplay(conf_matrix_liar_log, display_labels=model_log.classes_).plot(values_format='d')\n",
    "plt.title('LIAR Logistic Regression')\n",
    "\n",
    "ConfusionMatrixDisplay(conf_matrix_liar_svm, display_labels=model_svm.classes_).plot(values_format='d')\n",
    "plt.title('LIAR Support Vector Machine')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fake_News_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
